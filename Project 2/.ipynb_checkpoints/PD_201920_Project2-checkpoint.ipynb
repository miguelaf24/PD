{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining / Prospecção de Dados\n",
    "\n",
    "## Sara C. Madeira and André Falcão, 2019/20\n",
    "\n",
    "# Project 2 - Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Logistics\n",
    "\n",
    "**In a \"normal\" scenario students should work in teams of 2 people. Due to the social distance imposed by current public health situation, students were allowed to work in groups of 1 and 3. In this context, the amount of work was adapted according to the number of students in groups as described below.**\n",
    "\n",
    "* Tasks **1 to 5** should be done by **all** groups\n",
    "* Task **6** should be done only by **groups of 2 and 3** students\n",
    "* Task **7** should be done only by **groups of 3** students\n",
    "\n",
    "The quality of the project will then dictate its grade.\n",
    "\n",
    "**The project's solution should be uploaded in Moodle before the end of May, 17th 2020 (23:59).** \n",
    "\n",
    "**It is mandatory to produce a Jupyter notebook containing code and text/images/tables/etc describing the solution and the results. Projects not delivered in this format will not be graded. Note that you can use `PD_201920_Project2.ipynb`as template.**\n",
    "\n",
    "Students should **upload a `.zip` file** containing all the files necessary for project evaluation. \n",
    "\n",
    "**Decisions should be justified and results should be critically discussed.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Tools\n",
    "\n",
    "In this project you should use [Python 3](https://www.python.org), [Jupyter Notebook](http://jupyter.org) and **[Scikit-learn](http://scikit-learn.org/stable/)**.\n",
    "\n",
    "The dataset to be analysed is **`AML_ALL_PATIENTS_GENES_EXTENDED.csv`**. This is an extended version of the widely studied **Leukemia dataset**, originally published by Golub et al. (1999) [\"Molecular Classification of Cancer: Class Discovery and Class Prediction by Gene\n",
    "Expression Monitoring\"](http://archive.broadinstitute.org/mpr/publications/projects/Leukemia/Golub_et_al_1999.pdf.) \n",
    "\n",
    "**This dataset studies patients with leukaemia. At disease onset clinicials diagnosed them in two different types of leukaemia: acute myeloid leukemia (AML) and acute lymphoblastic leukemia (ALL).** Some of these diagnoses were later confirmed, other revealed to be wrong. The data analyzed here contains the expression levels of 5147 Human genes (features/columns) analyzed in 110 patients (rows): 70 ALL and 40 AML.\n",
    "Each row identifies a patient: The first column, `ID`, contains the patients' IDs , the second column, `DIAGNOSIS`, contains the initial diagnosis as performed by clinicians (ground truth), and the remaining 5147 columns contain the expression levels of the 5147 genes analysed.\n",
    "\n",
    "**The goal is to cluster patients and (ideally) find AML groups and ALL groups.**\n",
    "\n",
    "\n",
    "<img src=\"AML_ALL_PATIENTS_GENES_EXTENDED.jpg\" alt=\"AML_ALL_PATIENTS_GENES_EXTENDED.csv\" style=\"width: 1000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Preprocess Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of this step you should have:\n",
    "* a 110 rows × 5147 columns matrix, **X**, containing the values of the 5147 features for each of the 110 patients.\n",
    "* a vector, **y**, with the 110 diagnosis, which you can use later to evaluate clustering quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "import plotly.offline as offline\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('AML_ALL_PATIENTS_GENES_EXTENDED.csv')\n",
    "X = df.iloc[:,2:]\n",
    "y = df.iloc[:,1]\n",
    "y = y.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Verificação se existem colunas com valores ``null``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X.columns[X.isna().any()].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Não foram encontrados valores `null`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Verificação se existem colunas com valores ``0``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "zeros_columns = (X == 0).astype(int).sum(axis=0)\n",
    "zeros_columns = zeros_columns.to_frame()\n",
    "zeros_columns = zeros_columns[zeros_columns[0]>0]\n",
    "zeros_columns.sort_values(by = [0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#for row in zeros_columns.index: \n",
    "    #print(row, end = \", \") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Os genes ``AC002073_cds1_at, M16801_at, M25756_at, U07418_at, U13044_at, U27459_at, U69141_at, X51405_at, X66079_at, U10690_f_at`` têm valores iguais a zero que podem ser dados que não foram preenchidos. \n",
    "   \n",
    "Caso que estes valores iguais a zero sejam valores em falta, estes valores fazem com que exista um desvio dos valores reais, alterando os valores da média e a mediana.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Verificação quais as entradas com valores a ``0``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "zeros_rows = (X == 0).astype(int).sum(axis=1)\n",
    "zeros_rows = zeros_rows.to_frame()\n",
    "zeros_rows = zeros_rows[zeros_rows[0]>0]\n",
    "zeros_rows.sort_values(by = [0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Foram entcontrados valores iguais a zero nos seguintes pacientes:\n",
    "    6, 7, 17, 19, 38, 45, 47, 60, 61, 66, 73, 77, 85\n",
    "\n",
    "Onde os pacientes:\n",
    "- 7, 66 têm **3** valores a zero,\n",
    "- 45, 47 têm **2** valores a zero\n",
    "- 6, 17, 19, 38, 60, 61, 73, 77, 85 têm **1** valor a zero "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you already noticed the number of features (genes) is extremely high whe compared to the number of objects to cluster (patients). In this context, you should perform dimensionality reduction, that is, reduce the number of features, in two ways:\n",
    "\n",
    "* [**Removing features with low variance**](http://scikit-learn.org/stable/modules/feature_selection.html)\n",
    "\n",
    "* [**Using Principal Component Analysis**](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)\n",
    "\n",
    "At the end of this step you should have two new matrices with the same number of rows, each with a different number of columns (features): **X_variance** and **X_PCA**. \n",
    "\n",
    "**Don't change X you will need it!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Low variance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "def variance_threshold_selector(data, threshold=0.5):\n",
    "    selector = VarianceThreshold(threshold)\n",
    "    selector.fit(data)\n",
    "    return data[data.columns[selector.get_support(indices=True)]]\n",
    "\n",
    "X_variance = variance_threshold_selector(data = X, threshold=1.2)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Foi reduzido para %d features\" % X_variance.shape[1])\n",
    "print()\n",
    "\n",
    "zeros_columns.index.tolist() in X_variance.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'PCA is effected by scale so you need to scale the features in your data before applying PCA.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma vez que todas as features são genes humanas, à partida estarão na mesma escala e não haverá necessidade de utilizar StandardScaler no entanto de forma a procurar uma melhor performance dos algoritmos aplicamos este método de preprocessamento de modo a obter média = 0 e variância = 1. Que são os valores que obtem melhores resultados nos demais algortimos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Análise das componentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca = pca.fit(X, y)\n",
    "\n",
    "eigenvalues = pca.singular_values_\n",
    "\n",
    "kaiser_pca = len(list(filter(lambda a: a > 1, eigenvalues)))\n",
    "\n",
    "print(\"Kaiser Rule: nº PC = \", kaiser_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_exp = pca.explained_variance_ratio_\n",
    "cum_var_exp = np.cumsum(var_exp)\n",
    "xi = [\"PC%s\" %i for i in range(1,11)]\n",
    "trace1 = go.Scatter(\n",
    "    x=xi,\n",
    "    y=list(var_exp),\n",
    "    name=\"Explained Variance\")\n",
    "\n",
    "trace2 = go.Scatter(\n",
    "    x=xi,\n",
    "    y=cum_var_exp,\n",
    "    name=\"Cumulative Variance\")\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='Explained variance',\n",
    "    xaxis=dict(title='Principle Components', tickmode='linear'))\n",
    "\n",
    "data = [trace2, trace1]\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Através do gráfico podemos verificar que depois da Componente principal 2 não existe grande variação na explicação da variância. Segundo a regra do \"elbow\" 2 componentes seriam o ideal, apesar de só explicarem 27% da variância, como podemos verificar no gráfico apartir da \"Cumulative Variance\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se quisermos obter algo mais rigoroso poderemos olhar para a \"Cumulative Variance\" e escolher uma percentagem mais satisfatória por exemplo 9 Componentes Principais que representam 50% da variância"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  PCA (n_components = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### Sem StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "\n",
    "principalComponents = pca.fit_transform(X)\n",
    "\n",
    "\n",
    "X_PCA = pd.DataFrame(data = principalComponents, columns = ['principal component 1', 'principal component 2'])\n",
    "\n",
    "X_PCA = pd.concat([X_PCA, y], axis = 1)\n",
    "\n",
    "X_PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "ax.set_title('2 component PCA', fontsize = 20)\n",
    "targets = ['ALL', 'AML']\n",
    "colors = ['r', 'g']\n",
    "for DIAGNOSIS, color in zip(targets,colors):\n",
    "    indicesToKeep = X_PCA['DIAGNOSIS'] == DIAGNOSIS\n",
    "    ax.scatter(X_PCA.loc[indicesToKeep, 'principal component 1']\n",
    "               , X_PCA.loc[indicesToKeep, 'principal component 2']\n",
    "               , c = color\n",
    "               , s = 50)\n",
    "ax.legend(targets)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### Com StandardScaler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Standerizar as features\n",
    "x_scaler = StandardScaler().fit_transform(X)\n",
    "\n",
    "#Processo igual ao anterior\n",
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(x_scaler)\n",
    "X_PCA_scaler = pd.DataFrame(data = principalComponents, columns = ['principal component 1', 'principal component 2'])\n",
    "\n",
    "X_PCA_scaler_y = pd.concat([X_PCA_scaler, y], axis = 1)\n",
    "\n",
    "X_PCA_scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "É possível observar através do plot que apesar de dar a sensação dos pontos continuarem próximos as suas posições mudaram e tendo em conta a escala, que diminui considerávelmente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "ax.set_title('2 component PCA', fontsize = 20)\n",
    "targets = ['ALL', 'AML']\n",
    "colors = ['r', 'g']\n",
    "for DIAGNOSIS, color in zip(targets,colors):\n",
    "    indicesToKeep = X_PCA['DIAGNOSIS'] == DIAGNOSIS\n",
    "    ax.scatter(X_PCA_scaler.loc[indicesToKeep, 'principal component 1']\n",
    "               , X_PCA_scaler.loc[indicesToKeep, 'principal component 2']\n",
    "               , c = color\n",
    "               , s = 50)\n",
    "ax.legend(targets)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Write text in cells like this ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA Kaiser Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_Kaiser = PCA(n_components=kaiser_pca)\n",
    "\n",
    "principalComponentsKaiser = pca_Kaiser.fit(X)#.tolist()\n",
    "\n",
    "X_PCA_Kaiser = principalComponentsKaiser.transform(X)\n",
    "#X_PCA10 = pd.DataFrame(data = principalComponents10, columns = ['principal component 1', 'principal component 2'])\n",
    "\n",
    "#X_PCA10 = pd.concat([X_PCA10, y], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 3. Clustering Patients using Partitional Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Use **`K`-means** to cluster the patients:\n",
    "\n",
    "* Cluster the original data (5147 features): **X**.\n",
    "    * Use different values of `K`.\n",
    "    * For each value of `K` present the clustering by specifying how many patients ALL and AML are in each cluster.     \n",
    "    For instance, `{0: {'ALL': 70, 'AML': 0}, 1: {'ALL': 0, 'AML': 40}}` is the ideal clustering that we aimed at obtained with K-means when `K=2`, where the first cluster has 70 ALL patients and 0 AML patients and the second cluster has 0 ALL patients and 40 AML patients. \n",
    "    You can choose how to output this information.  \n",
    "    * What is the best value of `K` ? Justify using the clustering results and the [Silhouette score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html).\n",
    "\n",
    "* Cluster the data obtained after removing features with low variance: **X_variance**.\n",
    "    * Study different values of `K` as above.\n",
    "\n",
    "* Cluster the data obtained after applying PCA: **X_PCA**.\n",
    "    * Study different values of `K` as above.\n",
    "\n",
    "* Compare the results obtained in the three datasets above for the best `K`. Discuss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Implementámos o método \"Elbow\" para ter inicialmente uma ideia de quais os melhores valores para k. \n",
    "No entanto frizamos, inicialmente, uma vez que vamos concluir o melhor valor de k com o recurso às várias performances obtidas com os vários valores assim como da 'silhouette score'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%javascript  \n",
    "IPython.OutputArea.auto_scroll_threshold = 9999;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def f_kmeans(data, range_n_clusters):\n",
    "    if type(data) == type(X):\n",
    "        data = np.array(data.values.tolist())\n",
    "    \n",
    "    list_silhouette_avg = []\n",
    "    list_cluster_labels = []\n",
    "    \n",
    "    for n_clusters in range_n_clusters:\n",
    "\n",
    "\n",
    "        clusterer = KMeans(n_clusters=n_clusters, init = 'random', max_iter=10000, n_init=10,random_state=0)\n",
    "        cluster_labels = clusterer.fit_predict(data)\n",
    "        list_cluster_labels.append(cluster_labels)\n",
    "        \n",
    "        silhouette_avg = silhouette_score(data, cluster_labels)\n",
    "        list_silhouette_avg.append(silhouette_avg)\n",
    "        print(\"For n_clusters =\", n_clusters,\n",
    "              \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "        if data.shape[1] == 2 :\n",
    "\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "            fig.set_size_inches(18, 7)\n",
    "\n",
    "            ax1.set_xlim([-0.1, 1])\n",
    "            ax1.set_ylim([0, len(data) + (n_clusters + 1) * 10])\n",
    "\n",
    "            sample_silhouette_values = silhouette_samples(data, cluster_labels)\n",
    "\n",
    "            y_lower = 10\n",
    "            for i in range(n_clusters):\n",
    "                # Aggregate the silhouette scores for samples belonging to\n",
    "                # cluster i, and sort them\n",
    "                ith_cluster_silhouette_values = \\\n",
    "                    sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "                ith_cluster_silhouette_values.sort()\n",
    "\n",
    "                size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "                y_upper = y_lower + size_cluster_i\n",
    "\n",
    "                color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "                ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                                  0, ith_cluster_silhouette_values,\n",
    "                                  facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "                ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "                y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "            ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "            ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "            ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "            ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "            ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "            ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "            # segundo Plot\n",
    "            colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "            ax2.scatter(data[:, 0], data[:, 1], marker='.', s=30, lw=0, alpha=0.7, c=colors, edgecolor='k')\n",
    "\n",
    "            centers = clusterer.cluster_centers_\n",
    "\n",
    "            ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
    "                        c=\"white\", alpha=1, s=200, edgecolor='k')\n",
    "\n",
    "            for i, c in enumerate(centers):\n",
    "                ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n",
    "                            s=50, edgecolor='k')\n",
    "\n",
    "            ax2.set_title(\"The visualization of the clustered data.\")\n",
    "            ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "            ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "            plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                          \"with n_clusters = %d\" % n_clusters),\n",
    "                         fontsize=14, fontweight='bold')\n",
    "\n",
    "            plt.show()\n",
    "    return list_silhouette_avg, list_cluster_labels, range_n_clusters\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def result_kmeans(data, y, range_n_cluster = [2, 3, 4, 5, 6]):\n",
    "    \n",
    "    list_silhouette_avg, list_cluster_labels, range_n_clusters = f_kmeans(data, range_n_cluster)\n",
    "    \n",
    "    kmeans_results = y\n",
    "    for i in range(len(range_n_clusters)):\n",
    "        print(\"\\n\\nK = \",range_n_clusters[i], \"\\n\")\n",
    "        kmeans_results[str(range_n_clusters[i])] = list_cluster_labels[i]\n",
    "\n",
    "        df_aux = kmeans_results[kmeans_results.columns[[0,1+i]]]\n",
    "        score = 0\n",
    "        for j in range(0,range_n_clusters[i]):\n",
    "            all_count = df_aux[df_aux[str(range_n_clusters[i])]==j][\"DIAGNOSIS\"].str.count('ALL').sum()\n",
    "            aml_count = df_aux[df_aux[str(range_n_clusters[i])]==j][\"DIAGNOSIS\"].str.count('AML').sum()\n",
    "            aux=0\n",
    "            if(all_count>=aml_count):\n",
    "                aux = all_count\n",
    "            else:\n",
    "                aux = aml_count\n",
    "            print(j, \": {ALL =\",all_count, \", AML = \",aml_count,\"}\", \"score:\", (aux/(all_count+aml_count)))\n",
    "            if ~(np.isnan(aux/(all_count+aml_count))):\n",
    "                score += (all_count+aml_count)*(aux/(all_count+aml_count))\n",
    "        print(\"Score final: \", score/len(y))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### All Features Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "result_kmeans(X, y, range(2,21))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### PCA(n_components = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_PCA = X_PCA.drop([\"DIAGNOSIS\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "result_kmeans(X_PCA, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### PCA Kaiser Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "result_kmeans(X_PCA_Kaiser, y, list(range(2,21)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Features with more Variance (Low Variance remove features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "result_kmeans(X_variance, y, range(2,21))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Ivo Things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "wcss=[]\n",
    "\n",
    "for i in range(1,71):\n",
    "    kmeans = KMeans(n_clusters=i, init ='k-means++', max_iter=300,  n_init=10,random_state=0)\n",
    "    kmeans.fit(X)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "    \n",
    "plt.plot(range(1,71),wcss)\n",
    "plt.title('The Elbow Method Graph')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fviz_nbclust(X, kmeans, method = \"wss\", k.max = 110) + theme_minimal() + ggtitle(\"the Elbow Method\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X[X.columns[[0,1]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### k = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_teste = StandardScaler().fit_transform(X)\n",
    "colors = ['red', 'blue', 'cyan', 'orange', 'green']\n",
    "\n",
    "def k_means(clusters, data):\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=clusters, init = 'random', max_iter=1000, n_init=10,random_state=0 )\n",
    "    y_kmeans = kmeans.fit_predict(data)\n",
    "    \n",
    "    for i in range(0,clusters):\n",
    "        plt.scatter(np.array(data.values.tolist())[y_kmeans==i, 0], np.array(data.values.tolist())[y_kmeans==i, 1], s=50, c=colors[i], label ='Cluster 1')\n",
    "\n",
    "\n",
    "    plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='black', marker='x', label = 'Centroids')\n",
    "\n",
    "    plt.show()\n",
    "    return(y_kmeans)\n",
    "#k_means(2, x_teste)\n",
    "y2_kmeans = k_means(2, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#from collections import Counter\n",
    "\n",
    "#counter_k_2 = Counter(y_kmeans).values()\n",
    "\n",
    "\n",
    "def renomear (row):\n",
    "   if row['DIAGNOSIS'] == 'ALL' :\n",
    "      return 1\n",
    "   return 0\n",
    "\n",
    "labels = y.apply (lambda row: renomear(row), axis=1)\n",
    "\n",
    "ALL=0\n",
    "AML=0\n",
    "ALL_1=0\n",
    "AML_1=0\n",
    "\n",
    "for i in range(0,110):\n",
    "    if y2_kmeans[i] == 0 & labels[i] == 1:\n",
    "        ALL = ALL + 1\n",
    "    elif y2_kmeans[i] == 0 & labels[i] == 0:\n",
    "        AML = AML + 1\n",
    "    elif y2_kmeans[i] == 1 & labels[i] == 1:\n",
    "        ALL_1 = ALL_1 + 1\n",
    "    else:\n",
    "        AML_1 = AML_1 + 1\n",
    "\n",
    "#{0: {'ALL': 70, 'AML': 0}, 1: {'ALL': 0, 'AML': 40}}\n",
    "print('K = 2')\n",
    "print('0: {ALL: %s, AML: %s}, 1: {ALL: %s, AML: %s}' % (ALL,AML,ALL_1,AML_1))\n",
    "\n",
    "\n",
    "y2_kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### k = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "'''kmeans = KMeans(n_clusters=3, init = 'random', max_iter=1000, n_init=10,random_state=0 )\n",
    "y_kmeans = kmeans.fit_predict(x_teste)\n",
    "\n",
    "plt.scatter(x_teste[y_kmeans==0, 0], x_teste[y_kmeans==0, 1], s=100, c='red', label ='Cluster 1')\n",
    "plt.scatter(x_teste[y_kmeans==1, 0], x_teste[y_kmeans==1, 1], s=100, c='blue', label ='Cluster 2')\n",
    "plt.scatter(x_teste[y_kmeans==2, 0], x_teste[y_kmeans==2, 1], s=100, c='cyan', label ='Cluster 3')\n",
    "\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='yellow', label = 'Centroids')\n",
    "\n",
    "plt.show()'''\n",
    "\n",
    "k_means(3, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### k = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "'''kmeans = KMeans(n_clusters=4, init = 'random', max_iter=1000, n_init=10,random_state=0 )\n",
    "y_kmeans = kmeans.fit_predict(x_teste)\n",
    "\n",
    "plt.scatter(x_teste[y_kmeans==0, 0], x_teste[y_kmeans==0, 1], s=100, c='red', label ='Cluster 1')\n",
    "plt.scatter(x_teste[y_kmeans==1, 0], x_teste[y_kmeans==1, 1], s=100, c='blue', label ='Cluster 2')\n",
    "plt.scatter(x_teste[y_kmeans==2, 0], x_teste[y_kmeans==2, 1], s=100, c='cyan', label ='Cluster 3')\n",
    "plt.scatter(x_teste[y_kmeans==3, 0], x_teste[y_kmeans==3, 1], s=100, c='green', label ='Cluster 4')\n",
    "\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='yellow', label = 'Centroids')\n",
    "\n",
    "plt.show()'''\n",
    "k_means(4, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "'''kmeans = KMeans(n_clusters=5, init = 'random', max_iter=1000, n_init=10,random_state=0 )\n",
    "y_kmeans = kmeans.fit_predict(x_teste)\n",
    "\n",
    "plt.scatter(x_teste[y_kmeans==0, 0], x_teste[y_kmeans==0, 1], s=100, c='red', label ='Cluster 1')\n",
    "plt.scatter(x_teste[y_kmeans==1, 0], x_teste[y_kmeans==1, 1], s=100, c='blue', label ='Cluster 2')\n",
    "plt.scatter(x_teste[y_kmeans==2, 0], x_teste[y_kmeans==2, 1], s=100, c='cyan', label ='Cluster 3')\n",
    "plt.scatter(x_teste[y_kmeans==3, 0], x_teste[y_kmeans==3, 1], s=100, c='green', label ='Cluster 4')\n",
    "plt.scatter(x_teste[y_kmeans==4, 0], x_teste[y_kmeans==4, 1], s=100, c='orange', label ='Cluster 5')\n",
    "\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='yellow', label = 'Centroids')\n",
    "\n",
    "plt.show()'''\n",
    "k_means(5, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Cluster -> X_variance\n",
    "### k = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#X_variance = StandardScaler().fit_transform(X_variance) // não causa diferença uma vez que já foi aplicado o preprocessamento da variância\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, init = 'random', max_iter=1000, n_init=10,random_state=0 )\n",
    "y_kmeans = kmeans.fit_predict(X_variance)\n",
    "\n",
    "plt.scatter(X_variance[y_kmeans==0, 0], X_variance[y_kmeans==0, 1], s=100, c='red', label ='Cluster 1')\n",
    "plt.scatter(X_variance[y_kmeans==1, 0], X_variance[y_kmeans==1, 1], s=100, c='blue', label ='Cluster 2')\n",
    "\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='yellow', label = 'Centroids')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### K = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3, init = 'random', max_iter=1000, n_init=10,random_state=0 )\n",
    "y_kmeans = kmeans.fit_predict(X_variance)\n",
    "\n",
    "plt.scatter(X_variance[y_kmeans==0, 0], X_variance[y_kmeans==0, 1], s=100, c='red', label ='Cluster 1')\n",
    "plt.scatter(X_variance[y_kmeans==1, 0], X_variance[y_kmeans==1, 1], s=100, c='blue', label ='Cluster 2')\n",
    "plt.scatter(X_variance[y_kmeans==2, 0], X_variance[y_kmeans==2, 1], s=100, c='green', label ='Cluster 3')\n",
    "\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='yellow', label = 'Centroids')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### K = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=4, init = 'random', max_iter=1000, n_init=10,random_state=0 )\n",
    "y_kmeans = kmeans.fit_predict(X_variance)\n",
    "\n",
    "plt.scatter(X_variance[y_kmeans==0, 0], X_variance[y_kmeans==0, 1], s=100, c='red', label ='Cluster 1')\n",
    "plt.scatter(X_variance[y_kmeans==1, 0], X_variance[y_kmeans==1, 1], s=100, c='blue', label ='Cluster 2')\n",
    "plt.scatter(X_variance[y_kmeans==2, 0], X_variance[y_kmeans==2, 1], s=100, c='green', label ='Cluster 3')\n",
    "plt.scatter(X_variance[y_kmeans==3, 0], X_variance[y_kmeans==3, 1], s=100, c='orange', label ='Cluster 4')\n",
    "\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='yellow', label = 'Centroids')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### K = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=5, init = 'random', max_iter=1000, n_init=10,random_state=0 )\n",
    "y_kmeans = kmeans.fit_predict(X_variance)\n",
    "\n",
    "plt.scatter(X_variance[y_kmeans==0, 0], X_variance[y_kmeans==0, 1], s=100, c='red', label ='Cluster 1')\n",
    "plt.scatter(X_variance[y_kmeans==1, 0], X_variance[y_kmeans==1, 1], s=100, c='blue', label ='Cluster 2')\n",
    "plt.scatter(X_variance[y_kmeans==2, 0], X_variance[y_kmeans==2, 1], s=100, c='green', label ='Cluster 3')\n",
    "plt.scatter(X_variance[y_kmeans==3, 0], X_variance[y_kmeans==3, 1], s=100, c='orange', label ='Cluster 4')\n",
    "plt.scatter(X_variance[y_kmeans==4, 0], X_variance[y_kmeans==4, 1], s=100, c='cyan', label ='Cluster 5')\n",
    "\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='yellow', label = 'Centroids')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### X_PCA\n",
    "## k = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "k_means(2, X_PCA_scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### k = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "k_means(3, X_PCA_scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### k = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "k_means(4, X_PCA_scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "k_means(5, X_PCA_scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 4. Clustering Patients using Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Use a **Hierarchical Clustering Algorithm (HCA)** to cluster the patients: \n",
    "\n",
    "* Cluster the data in **X_variance**.\n",
    "    * Use **different linkage metrics**. (single, complete, average, ward) - eu é que acrescentei\n",
    "    * Use different values of `K`.\n",
    "    * For each linkage metric and value of `K` present the clustering by specifying how many patients ALL and AML are in each cluster as you did before. \n",
    "    * What is the best linkage metric and the best value of `K`? Justify using the clustering results and the [Silhouette score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html).\n",
    "\n",
    "* Cluster the data in **X_PCA**.\n",
    "    * Study different linkage metrics and different values of `K` as above.\n",
    "\n",
    "* Compare the results obtained in the two datasets above for the best linkage metric and the best `K`. Discuss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### X_variance - Hierarchical Clustering - FALTA CONTADOR/OUTRO E DISCUSSÃO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import scipy.cluster.hierarchy as shc\n",
    "\n",
    "#PLot\n",
    "plt.figure(figsize=(10, 7))  \n",
    "plt.title(\"Dendrograms\")  \n",
    "dend = shc.dendrogram(shc.linkage(X_variance, method='ward'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_variance = np.array(X_variance.values.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### k = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def hc(clusters, data, metric):\n",
    "    \n",
    "    cluster = AgglomerativeClustering(n_clusters=clusters, affinity='euclidean', linkage=metric)\n",
    "    y_hc = cluster.fit_predict(data)\n",
    "    \n",
    "    for i in range(0,clusters):\n",
    "        plt.scatter(X_variance[y_hc==i, 0], X_variance[y_hc==i, 1], s=100, c=colors[i], label ='Cluster 1')\n",
    "\n",
    "    plt.show()\n",
    "    return(y_hc)\n",
    "\n",
    "hc2 = hc(2, X_variance,'ward')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### k = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hc3 = hc(3, X_variance, 'ward')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### k = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hc4 = hc(4, X_variance, 'ward')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hc5 = hc(5, X_variance, 'ward')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### X_PCA\n",
    "## k = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pca_hc2 = hc(2, X_PCA_scaler, 'ward')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### k = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pca_hc3 = hc(3, X_PCA_scaler, 'ward')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### k = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pca_hc4 = hc(4, X_PCA_scaler, 'ward')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pca_hc5 = hc(5, X_PCA_scaler, 'ward')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Write text in cells like this ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 5. Evaluating Clustering Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this task you should compare the best results obtained using `K`-means and HCA \n",
    "1. **Without using ground truth**\n",
    "2. **Using ground truth (`DIAGNOSIS`)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 5.1. Without Using Ground Truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Choose one adequate measure** from those available by Sciki-learn (https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation) to evaluate the different clusterings. \n",
    "\n",
    "Discuss the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Write code in cells like this\n",
    "# ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Write text in cells like this ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 5.2. Using Ground Truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Choose one adequate measure** from those available by Sciki-learn (https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation) to evaluate the different clusterings. \n",
    "\n",
    "Discuss the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Write code in cells like this\n",
    "# ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Write text in cells like this ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 6. Clustering Patients using Density-based Clustering\n",
    "\n",
    "Use DBSCAN (https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html) or OPTICS (https://scikit-learn.org/stable/modules/generated/sklearn.cluster.OPTICS.html) to cluster the patients.\n",
    "\n",
    "Compare the results with those of K-means and HCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "dbscan = DBSCAN(eps=5, min_samples = 2)\n",
    "clusters = dbscan.fit_predict(X_variance)\n",
    "\n",
    "plt.scatter(X_variance[clusters==0, 0], X_variance[clusters==0, 1], s=100, c='red', label ='Cluster 1')\n",
    "plt.scatter(X_variance[clusters==1, 0], X_variance[clusters==1, 1], s=100, c='blue', label ='Cluster 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Write text in cells like this ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 7. Choose a Different Clustering Algorithm to Group the Patients\n",
    "\n",
    "Choose **a clustering algorithm** besides `K`-means, HCA and DBSCAN/OPTICS to cluster the patients. \n",
    "\n",
    "Justify your choice and compare the results with those of `K`-means, HCA and DBSCAN/OPTICS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Affinity Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write text in cells like this ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MeanShift\n",
    "\n",
    "ms = MeanShift()\n",
    "clusters = ms.fit_predict(X_PCA_scaler)\n",
    "\n",
    "plt.scatter(x_teste[clusters==0, 0], x_teste[clusters==0, 1], s=100, c='red', label ='Cluster 2')\n",
    "plt.scatter(x_teste[clusters==1, 0], x_teste[clusters==1, 1], s=100, c='blue', label ='Cluster 2')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
