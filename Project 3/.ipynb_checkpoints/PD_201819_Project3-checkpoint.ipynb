{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining / Prospecção de Dados\n",
    "\n",
    "## Sara C. Madeira, 2017/18\n",
    "\n",
    "# Project 3 - Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistics\n",
    "\n",
    "**Students are encouraged to work in groups of two**, although individual projects are allowed. \n",
    "\n",
    "**The project's solution should be uploaded in Moodle before the end of January, 7th 2018.** \n",
    "\n",
    "Students should upload a `.zip` file containing all the files necessary for project evaluation. Two deliverables are allowed:\n",
    "\n",
    "* **[Preferred]** Jupyter notebook containing code and text describing the solution and the results;\n",
    "* **[Alternative]** Python code implementing the solution and a report (`.pdf`) explaining how to execute the code and text describing the solution and the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Tools\n",
    "\n",
    "In this project you should use [Python 3](https://www.python.org), [Jupyter Notebook](http://jupyter.org) and **[Scikit-learn](http://scikit-learn.org/stable/)**.\n",
    "\n",
    "The dataset to be analysed is **`AML_ALL_PATIENTS_GENES_LARGE.csv`**. This is a modified version of the widely studied **Leukemia dataset**, originally published by Golub et al. (1999) [\"Molecular Classification of Cancer: Class Discovery and Class Prediction by Gene\n",
    "Expression Monitoring\"](http://archive.broadinstitute.org/mpr/publications/projects/Leukemia/Golub_et_al_1999.pdf. This dataset **studies patients with two different types of leukaemia: acute myeloid leukemia (AML) and acute lymphoblastic leukemia (ALL).** The data analyzed here contains the expression levels of 5147 Human genes (features/columns) analyzed in 72 patients (rows): 47 ALL and 25 AML.\n",
    "\n",
    "**The goal is to learn to classify patients as AML or ALL.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Preprocess Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of this step you should have:\n",
    "* a 72 rows × 5147 columns matrix, **X**, containing the values of the 5147 features for each of the 72 patients.\n",
    "* a vector, **y**, with the 72 diagnosis, which will be used to train the classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you already noticed the number of features (genes) is extremely high whe compared to the number patients. In this context, you should perform dimensionality reduction, that is, reduce the number of features, in two ways:\n",
    "* Removing features with low variance: remove features with variance lower than 1.\n",
    "* Using Principal Component Analysis: use a number of componentes equal to 71 (number of patients - 1).\n",
    "\n",
    "At the end of this step you should have two new matrices with the same number of rows, each with a different number of columns (features): **X_variance** and **X_pca** together with the vector **y** obtained above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Learning Simple Classifiers\n",
    "\n",
    "Train the following classifiers: K-Nearest Neighbor, decision tree and Naive Bayes. For each present the classification report and the confusion matrix in both train and test.\n",
    "\n",
    "## 3.1. Using Train and Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results using High-Variance Features (X_variance, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using **X_variance** and **y** you should obtain: **X_train, X_test, y_train and y_test**. Split the data into train and test set using 70% and 30% respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Present and Discuss your results here\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results using PCA Features (X_pca, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Using **X_pca** and **y** you should obtain: **X_pca_train, X_pca_test, y_pca_train and y_pca_test**. Split the data into train and test set using 70% and 30% respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Present and Discuss your results here\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Using Cross-Validation\n",
    "\n",
    "For the **best 2 classifiers** above present and discuss the results obtained using stratified 10 fold cross-validation. Use the mean and standard deviation of the accuracy, precision and recall in both train and test as evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Present and Discuss your results here\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Improving Classification Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Feature Selection\n",
    "\n",
    "Use **X_variance** and **y** and present the results for stratified 10 fold-cross validation (mean and standard deviation of accuracy, precision and recall) for the **best classifier** so far:\n",
    "* Try [Univariate Feature Selection](http://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection) using `mutual_info_classif` as scoring criterium. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Present and Discuss your results here\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Ensemble Learning\n",
    "\n",
    "Use **X_variance** and **y** and present the results for stratified 10 fold-cross validation (mean and standard deviation of accuracy, precision and recall) for the following 3 experiments:\n",
    "\n",
    "## Voting Classifier\n",
    "\n",
    "* Use a [voting classifier](http://scikit-learn.org/stable/modules/ensemble.html#voting-classifier) to combine the results of K-nearest neighbors, decision trees and a naive Bayes. \n",
    "\n",
    "## AdaBoost \n",
    "\n",
    "* Try [AdaBoost](http://scikit-learn.org/stable/modules/ensemble.html#adaboost).\n",
    "\n",
    "## Random Forests\n",
    "\n",
    "* Try [Random Forests](http://scikit-learn.org/stable/modules/ensemble.html#random-forests)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
